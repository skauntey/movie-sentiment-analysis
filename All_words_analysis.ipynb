{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Bert_version_analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skauntey/movie-sentiment-analysis/blob/ALMS/All_words_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LvNjbXGwGEn"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"kaunteyshah\",\"key\":\"e2678a9d0e44e057000b9daf059111b5\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-DSBh2oDB0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%% md\n"
        },
        "id": "0Vz1BA0xoDCW"
      },
      "source": [
        "#### 1. Downloading Kaggle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N6fajUBQoDCX"
      },
      "source": [
        "kaggle.api.authenticate()\n",
        "kaggle.api.competition_download_files('sentiment-analysis-on-movie-reviews', path= str(os.getcwd())+\"/dataset/\", force = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YsVVPfE6oDCa"
      },
      "source": [
        "#### 2. Unzipping Kaggle files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLzWG9BQsYd9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r7L-RkzGoDCd"
      },
      "source": [
        "import zipfile\n",
        "ziped_file = r'dataset/sentiment-analysis-on-movie-reviews.zip'\n",
        "folder_path = os.path.join(str(os.getcwd()), 'dataset/')\n",
        "for files in os.listdir(folder_path):\n",
        "  path = os.path.join(os.getcwd()+'/'+'dataset/',files)\n",
        "  if path.split('.')[-1] != \"zip\":\n",
        "    try:\n",
        "      shutil.rmtree(path)\n",
        "    except:\n",
        "      raise\n",
        "with zipfile.ZipFile(ziped_file) as zip_file:\n",
        "  for member in zip_file.namelist():\n",
        "      if member.split('.')[-1] == \"zip\":\n",
        "          fdir = member.split('.')[0]\n",
        "          zip_file.extract(member, path= os.path.join('dataset/'+fdir))\n",
        "          # extracting individual train and test files in their respective folders\n",
        "          zippedfile_name = os.listdir(os.path.join(str(os.getcwd()+'/dataset/'+fdir+'/')))\n",
        "          zippedfile = os.path.join(str(os.getcwd())+'/dataset/'+ fdir, zippedfile_name[0])\n",
        "          with zipfile.ZipFile(zippedfile, mode='r') as tsv_zip:\n",
        "              tsv_zip.extractall(path = os.path.join('dataset/'+fdir+'/'))\n",
        "                \n",
        "          os.remove(zippedfile)\n",
        "      else:\n",
        "        continue\n",
        "            \n",
        "os.remove(ziped_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XR-Gq7oRoDCg"
      },
      "source": [
        "#### 3. Testing files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cYK76UoboDCh"
      },
      "source": [
        "tsvfile = 'dataset/train/train.tsv'\n",
        "# read the data\n",
        "tsv_read = pd.read_csv(tsvfile, sep=\"\\t\")\n",
        "\n",
        "original_dataset = pd.DataFrame({ \n",
        "    'DATA_COLUMN': [tsv_read.loc[:,'Phrase'][x].lower() for x in range(len(tsv_read))], \n",
        "    'LABEL_COLUMN': np.array([tsv_read.loc[:,'Sentiment'][x] for x in range(len(tsv_read))], dtype='int32')},\n",
        "    columns= ['DATA_COLUMN','LABEL_COLUMN'])\n",
        "\n",
        "pdf_data = original_dataset['DATA_COLUMN']\n",
        "print(original_dataset['DATA_COLUMN'].head())\n",
        "print(pdf_data.head())\n",
        "pdf_labels = original_dataset['LABEL_COLUMN']\n",
        "print(type(pdf_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j5epneHA5sA"
      },
      "source": [
        "train_split = 0.8\n",
        "val_split = 0.2\n",
        "test_split = 0\n",
        "\n",
        "training_mark = int(abs(len(original_dataset) * train_split))\n",
        "# validation_mark = np.add(training_mark, int(abs(len(pdf)) * val_split))\n",
        "\n",
        "print(training_mark)\n",
        "# print(validation_mark)\n",
        "\n",
        "training_set = original_dataset.loc[:training_mark]\n",
        "train_data = training_set['DATA_COLUMN']\n",
        "train_labels = training_set['LABEL_COLUMN']\n",
        "validation_set = original_dataset.loc[training_mark:]\n",
        "val_data = validation_set['DATA_COLUMN']\n",
        "val_labels = validation_set['LABEL_COLUMN']\n",
        "# pdf.loc[training_mark:validation_mark]\n",
        "# test_set = np.array(pdf.loc[validation_mark:])\n",
        "\n",
        "print(train_labels.head())\n",
        "print(val_labels.head())\n",
        "print(type(val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zB5IeT9FVIT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyWv7L3Ft7bg"
      },
      "source": [
        "\"\"\"stopwords = [ \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"into\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def remove_stopwords(dataseries, original_dataset):\n",
        "  for word in stopwords:\n",
        "    tokens = [\" \"+word+\" \", \" \"+word, word+\" \"]\n",
        "    for token in tokens:\n",
        "      dataseries = [dataseries[i].replace(token, \" \") for i in range(len(dataseries))]\n",
        "      original_dataset['DATA_COLUMN'] = dataseries\n",
        "      new_pdf_data = original_dataset['DATA_COLUMN']\n",
        "  return dataseries, new_pdf_data\n",
        "\n",
        "_, new_pdf_data = remove_stopwords(pdf_data, original_dataset)\n",
        "\n",
        "print(new_pdf_data[0])\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9ejNlo3Fr7"
      },
      "source": [
        "\n",
        "def tokenizing_dataset(original_dataset):\n",
        "  tokenizer = Tokenizer(num_words = 16000, oov_token='<00V>')\n",
        "  tokenizer.fit_on_texts(original_dataset)\n",
        "  word_index = tokenizer.word_index\n",
        "  return word_index, tokenizer\n",
        "\n",
        "def Sequencing_dataset(original_dataset, dataset):\n",
        "  _, pretrained_tokenizer = tokenizing_dataset(original_dataset)\n",
        "  sequences = pretrained_tokenizer.texts_to_sequences(dataset)\n",
        "  padded_sentences = pad_sequences(sequences, maxlen= 50, truncating ='post', padding='post')\n",
        "  return padded_sentences, sequences\n",
        "\n",
        "padded_train, train_sequences = Sequencing_dataset(pdf_data, train_data)\n",
        "padded_val, val_sequences = Sequencing_dataset(pdf_data, val_data)\n",
        "print(max([len(x) for x in val_sequences]))\n",
        "print(padded_val[0])\n",
        "print(val_data[124848])\n",
        "print(padded_train[0])\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9sw8ipL5oDC0"
      },
      "source": [
        "def decoding_sequences(original_dataset, sequences):\n",
        "  word_index, _ = tokenizing_dataset(original_dataset)\n",
        "  reversed_index = dict(map(reversed, word_index.items()))\n",
        "  for letter in range(len(sequences)):\n",
        "      reversed_index.get(letter) \n",
        "  reversed_sequence = [reversed_index.get(letter) for letter in sequences]\n",
        "  return (' '.join(reversed_sequence))\n",
        "    \n",
        "print(decoding_sequences(pdf_data, train_sequences[0]))\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bip8i0ExoDC2"
      },
      "source": [
        "vocabulary_size = 16000\n",
        "embedding_dim = 128\n",
        "max_length = 50\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<00V>'\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             \n",
        "    tf.keras.layers.Embedding(vocabulary_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5, activation ='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), metrics= ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjTyKxhioDC3"
      },
      "source": [
        "num_epochs = 20\n",
        "\n",
        "history = model.fit(padded_train, train_labels, batch_size=10000, epochs = num_epochs, validation_data = (padded_val, val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXm7bhU0Kqyo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-mDU-mRNao"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDQgL14B8YLJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}