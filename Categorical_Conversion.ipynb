{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Bert_version_analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skauntey/movie-sentiment-analysis/blob/tf_tokenizer/Categorical_Conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LvNjbXGwGEn"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"kaunteyshah\",\"key\":\"e2678a9d0e44e057000b9daf059111b5\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-DSBh2oDB0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%% md\n"
        },
        "id": "0Vz1BA0xoDCW"
      },
      "source": [
        "#### 1. Downloading Kaggle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N6fajUBQoDCX"
      },
      "source": [
        "kaggle.api.authenticate()\n",
        "kaggle.api.competition_download_files('sentiment-analysis-on-movie-reviews', path= str(os.getcwd())+\"/dataset/\", force = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YsVVPfE6oDCa"
      },
      "source": [
        "#### 2. Unzipping Kaggle files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLzWG9BQsYd9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r7L-RkzGoDCd"
      },
      "source": [
        "import zipfile\n",
        "ziped_file = r'dataset/sentiment-analysis-on-movie-reviews.zip'\n",
        "folder_path = os.path.join(str(os.getcwd()), 'dataset/')\n",
        "for files in os.listdir(folder_path):\n",
        "  path = os.path.join(os.getcwd()+'/'+'dataset/',files)\n",
        "  if path.split('.')[-1] != \"zip\":\n",
        "    try:\n",
        "      shutil.rmtree(path)\n",
        "    except:\n",
        "      raise\n",
        "with zipfile.ZipFile(ziped_file) as zip_file:\n",
        "  for member in zip_file.namelist():\n",
        "      if member.split('.')[-1] == \"zip\":\n",
        "          fdir = member.split('.')[0]\n",
        "          zip_file.extract(member, path= os.path.join('dataset/'+fdir))\n",
        "          # extracting individual train and test files in their respective folders\n",
        "          zippedfile_name = os.listdir(os.path.join(str(os.getcwd()+'/dataset/'+fdir+'/')))\n",
        "          zippedfile = os.path.join(str(os.getcwd())+'/dataset/'+ fdir, zippedfile_name[0])\n",
        "          with zipfile.ZipFile(zippedfile, mode='r') as tsv_zip:\n",
        "              tsv_zip.extractall(path = os.path.join('dataset/'+fdir+'/'))\n",
        "                \n",
        "          os.remove(zippedfile)\n",
        "      else:\n",
        "        continue\n",
        "            \n",
        "os.remove(ziped_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XR-Gq7oRoDCg"
      },
      "source": [
        "#### 3. Testing files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cYK76UoboDCh"
      },
      "source": [
        "tsvfile = 'dataset/train/train.tsv'\n",
        "# read the data\n",
        "tsv_read = pd.read_csv(tsvfile, sep=\"\\t\")\n",
        "\n",
        "original_dataset = pd.DataFrame({ \n",
        "    'DATA_COLUMN': tsv_read['Phrase'], \n",
        "    'LABEL_COLUMN': tsv_read['Sentiment']})\n",
        "\n",
        "pdf_data = original_dataset['DATA_COLUMN']\n",
        "pdf_labels = original_dataset['LABEL_COLUMN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j5epneHA5sA"
      },
      "source": [
        "train_split = 0.9\n",
        "val_split = 0.1\n",
        "test_split = 0\n",
        "\n",
        "training_mark = int(abs(len(original_dataset) * train_split))\n",
        "# validation_mark = np.add(training_mark, int(abs(len(pdf)) * val_split))\n",
        "\n",
        "print(training_mark)\n",
        "# print(validation_mark)\n",
        "\n",
        "training_set = original_dataset.loc[:training_mark,:]\n",
        "validation_set = original_dataset.loc[training_mark:,:]\n",
        "\n",
        "train_data = training_set['DATA_COLUMN']\n",
        "train_labels = training_set['LABEL_COLUMN']\n",
        "\n",
        "val_data = validation_set['DATA_COLUMN']\n",
        "val_labels = validation_set['LABEL_COLUMN']\n",
        "\n",
        "print(train_labels.head())\n",
        "print(val_labels.head())\n",
        "\n",
        "print(train_data.head())\n",
        "print(train_labels.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zB5IeT9FVIT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyWv7L3Ft7bg"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\"\n",
        "    text: a string\n",
        "    return: modified initial string\n",
        "  \"\"\"\n",
        "  text = text.lower() # lowercase text\n",
        "  text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "  text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "  text = text.replace('x', '')\n",
        "#    text = re.sub(r'\\W+', '', text)\n",
        "  text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9ejNlo3Fr7"
      },
      "source": [
        "\n",
        "def tokenizing_dataset(original_dataset):\n",
        "  tokenizer = Tokenizer(num_words = 15290, oov_token='<00V>')\n",
        "  tokenizer.fit_on_texts(original_dataset)\n",
        "  word_index = tokenizer.word_index\n",
        "  return word_index, tokenizer\n",
        "\n",
        "def Sequencing_dataset(original_dataset, dataset):\n",
        "  _, pretrained_tokenizer = tokenizing_dataset(original_dataset)\n",
        "  sequences = pretrained_tokenizer.texts_to_sequences(dataset)\n",
        "  padded_sentences = pad_sequences(sequences, maxlen= 49, truncating ='post', padding='post')\n",
        "  return padded_sentences, sequences\n",
        "\n",
        "padded_train, train_sequences = Sequencing_dataset(pdf_data, train_data)\n",
        "padded_val, val_sequences = Sequencing_dataset(pdf_data, val_data)\n",
        "word_index_pdf, pdf_tokenizer = tokenizing_dataset(pdf_data)\n",
        "\n",
        "\n",
        "print(len(word_index_pdf))\n",
        "print(max([len(x) for x in train_sequences]))\n",
        "print(padded_val[0])\n",
        "print(padded_train[0])\n",
        "print(train_data[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9sw8ipL5oDC0"
      },
      "source": [
        "def decoding_sequences(original_dataset, sequences):\n",
        "  word_index, _ = tokenizing_dataset(original_dataset)\n",
        "  reversed_index = dict(map(reversed, word_index.items()))\n",
        "  for letter in range(len(sequences)):\n",
        "      reversed_index.get(letter) \n",
        "  reversed_sequence = [reversed_index.get(letter) for letter in sequences]\n",
        "  return (' '.join(reversed_sequence))\n",
        "    \n",
        "print(decoding_sequences(pdf_data, train_sequences[0]))\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bip8i0ExoDC2"
      },
      "source": [
        "vocabulary_size = len(word_index_pdf) + 1\n",
        "embedding_dim = 128\n",
        "max_length = 49\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<00V>'\n",
        "\n",
        "\"\"\"model = tf.keras.Sequential([\n",
        "                             \n",
        "    tf.keras.layers.Embedding(vocabulary_size, embedding_dim, input_length = max_length),\n",
        "    #tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=False, dropout = 0.2),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5, activation ='sigmoid')\n",
        "])\"\"\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index_pdf)+1, embedding_dim, input_length=max_length))\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
        "model.add(LSTM(30))\n",
        "\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=optimizer, metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjTyKxhioDC3"
      },
      "source": [
        "num_epochs = 30\n",
        "batch_size = 10000\n",
        "history = model.fit(padded_train, train_labels, batch_size=batch_size, epochs = num_epochs, validation_data=(padded_val, val_labels), validation_batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXm7bhU0Kqyo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-mDU-mRNao"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDQgL14B8YLJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}