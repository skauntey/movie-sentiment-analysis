{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Categoricalentropy_conversion.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skauntey/movie-sentiment-analysis/blob/tf_tokenizer/Categoricalentropy_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LvNjbXGwGEn"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"kaunteyshah\",\"key\":\"e2678a9d0e44e057000b9daf059111b5\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-DSBh2oDB0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import kaggle\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%% md\n"
        },
        "id": "0Vz1BA0xoDCW"
      },
      "source": [
        "#### 1. Downloading Kaggle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N6fajUBQoDCX"
      },
      "source": [
        "kaggle.api.authenticate()\n",
        "kaggle.api.competition_download_files('sentiment-analysis-on-movie-reviews', path= str(os.getcwd())+\"/dataset/\", force = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YsVVPfE6oDCa"
      },
      "source": [
        "#### 2. Unzipping Kaggle files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLzWG9BQsYd9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r7L-RkzGoDCd"
      },
      "source": [
        "import zipfile\n",
        "ziped_file = r'dataset/sentiment-analysis-on-movie-reviews.zip'\n",
        "folder_path = os.path.join(str(os.getcwd()), 'dataset/')\n",
        "for files in os.listdir(folder_path):\n",
        "  path = os.path.join(os.getcwd()+'/'+'dataset/',files)\n",
        "  if path.split('.')[-1] != \"zip\":\n",
        "    try:\n",
        "      shutil.rmtree(path)\n",
        "    except:\n",
        "      raise\n",
        "with zipfile.ZipFile(ziped_file) as zip_file:\n",
        "  for member in zip_file.namelist():\n",
        "      if member.split('.')[-1] == \"zip\":\n",
        "          fdir = member.split('.')[0]\n",
        "          zip_file.extract(member, path= os.path.join('dataset/'+fdir))\n",
        "          # extracting individual train and test files in their respective folders\n",
        "          zippedfile_name = os.listdir(os.path.join(str(os.getcwd()+'/dataset/'+fdir+'/')))\n",
        "          zippedfile = os.path.join(str(os.getcwd())+'/dataset/'+ fdir, zippedfile_name[0])\n",
        "          with zipfile.ZipFile(zippedfile, mode='r') as tsv_zip:\n",
        "              tsv_zip.extractall(path = os.path.join('dataset/'+fdir+'/'))\n",
        "                \n",
        "          os.remove(zippedfile)\n",
        "      else:\n",
        "        continue\n",
        "            \n",
        "os.remove(ziped_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XR-Gq7oRoDCg"
      },
      "source": [
        "#### 3. Testing files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cYK76UoboDCh"
      },
      "source": [
        "tsvfile = 'dataset/train/train.tsv'\n",
        "# read the data\n",
        "tsv_read = pd.read_csv(tsvfile, sep=\"\\t\")\n",
        "\n",
        "original_dataset = pd.DataFrame({ \n",
        "    'DATA_COLUMN': tsv_read['Phrase'], \n",
        "    'LABEL_COLUMN': tsv_read['Sentiment']})\n",
        "\n",
        "pdf_data = original_dataset['DATA_COLUMN']\n",
        "pdf_labels = original_dataset['LABEL_COLUMN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6rNN_WgkrDp"
      },
      "source": [
        "sorted_data = original_dataset['LABEL_COLUMN'].value_counts().sort_values(ascending=False)\n",
        "sorted_data.plot(kind = 'bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS6WHlHCkphM"
      },
      "source": [
        "import re\n",
        "original_dataset = original_dataset.reset_index(drop=True)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\"\n",
        "    text: a string\n",
        "    return: modified initial string\n",
        "  \"\"\"\n",
        "  text = text.lower() # lowercase text\n",
        "  text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "  text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "  text = text.replace('x', '')\n",
        "  text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "  return text\n",
        "\n",
        "\n",
        "original_dataset['DATA_COLUMN'] = original_dataset['DATA_COLUMN'].apply(clean_text)\n",
        "original_dataset.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi6o70-zpdnt"
      },
      "source": [
        "original_dataset.head()\n",
        "pdf_dataset = original_dataset[original_dataset['DATA_COLUMN'].astype(bool)]\n",
        "pdf_dataset = pdf_dataset.reset_index(drop=True)\n",
        "pdf_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j5epneHA5sA"
      },
      "source": [
        "train_split = 0.8\n",
        "val_split = 0.2\n",
        "test_split = 0\n",
        "\n",
        "training_mark = int(abs(len(pdf_dataset) * train_split))\n",
        "# validation_mark = np.add(training_mark, int(abs(len(pdf)) * val_split))\n",
        "\n",
        "print(training_mark)\n",
        "# print(validation_mark)\n",
        "\n",
        "# Based on Data and Labels across the whole dataset\n",
        "pdf_dataset_data = pdf_dataset['DATA_COLUMN'].values\n",
        "pdf_dataset_labels = pdf_dataset['LABEL_COLUMN'].values\n",
        "\n",
        "# One Hot Vector of the label dataset\n",
        "\n",
        "pdf_dataset_labels_onehot = pd.get_dummies(pdf_dataset_labels).values\n",
        "print('Shape of label tensor:', pdf_dataset_labels_onehot.shape)\n",
        "\n",
        "# Train/Test split without One_hot\n",
        "training_set = pdf_dataset[1:training_mark]\n",
        "validation_set = pdf_dataset[training_mark:]\n",
        "\n",
        "\n",
        "train_data = training_set['DATA_COLUMN']\n",
        "train_labels = training_set['LABEL_COLUMN']\n",
        "\n",
        "val_data = validation_set['DATA_COLUMN']\n",
        "val_labels = validation_set['LABEL_COLUMN']\n",
        "\n",
        "print(val_data.head())\n",
        "print(val_labels.head())\n",
        "\n",
        "print(train_data.head())\n",
        "print(train_labels.head())\n",
        "\n",
        "print(pdf_dataset_data[:10])\n",
        "print(type(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zB5IeT9FVIT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NS7kTDngW1P"
      },
      "source": [
        "\"\"\"The sentiment labels are:\n",
        "\n",
        "0 - negative\n",
        "1 - somewhat negative\n",
        "2 - neutral\n",
        "3 - somewhat positive\n",
        "4 - positive\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwxpvieC45wJ"
      },
      "source": [
        "\"\"\"def num_words (key_dataset):\n",
        "  total_words = {}\n",
        "  for x in key_dataset:\n",
        "    for a in x.split():\n",
        "      if a not in total_words.keys():\n",
        "        total_words[a] = 1\n",
        "      else:\n",
        "        total_words[a] += 1\n",
        "  return len(total_words.keys())\n",
        "\n",
        "num_words = num_words(pdf_dataset_data)\n",
        "num_words\"\"\"\n",
        "#16245"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p51B8pReMCoo"
      },
      "source": [
        "\"\"\"def max_len(key_dataset):\n",
        "  _, pretrained_tokenizer = tokenizing_dataset(key_dataset)\n",
        "  sequences = pretrained_tokenizer.texts_to_sequences(key_dataset)\n",
        "  max_sentence = max([len(x) for x in sequences])\n",
        "  return max_sentence\n",
        "\n",
        "max_len = max_len(pdf_dataset_data)\n",
        "max_len\"\"\"\n",
        "#31"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9ejNlo3Fr7"
      },
      "source": [
        "\n",
        "\n",
        "def max_len(key_dataset):\n",
        "  _, pretrained_tokenizer = tokenizing_dataset(key_dataset)\n",
        "  sequences = pretrained_tokenizer.texts_to_sequences(key_dataset)\n",
        "  max_sentence = max([len(x) for x in sequences])\n",
        "  return max_sentence\n",
        "\n",
        "\n",
        "def tokenizing_dataset(key_dataset):\n",
        "  tokenizer = Tokenizer(num_words = 16245,oov_token='<00V>')\n",
        "  tokenizer.fit_on_texts(key_dataset)\n",
        "  word_index = tokenizer.word_index\n",
        "  return word_index, tokenizer\n",
        "\n",
        "def Sequencing_dataset(key_dataset, dataset):\n",
        "  _, pretrained_tokenizer = tokenizing_dataset(key_dataset)\n",
        "  sequences = pretrained_tokenizer.texts_to_sequences(dataset)\n",
        "  padded_sentences = pad_sequences(sequences, maxlen= 31, truncating ='post', padding='post')\n",
        "  return padded_sentences, sequences\n",
        "\n",
        "\n",
        "padded_train, train_sequences = Sequencing_dataset(pdf_dataset_data, train_data)\n",
        "padded_val, val_sequences = Sequencing_dataset(pdf_dataset_data, val_data)\n",
        "word_index_pdf, pdf_tokenizer = tokenizing_dataset(pdf_dataset_data)\n",
        "\n",
        "print(len(word_index_pdf))\n",
        "print(max([len(x) for x in train_sequences]))\n",
        "print(padded_val[0])\n",
        "print(padded_train[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9sw8ipL5oDC0"
      },
      "source": [
        "def decoding_sequences(original_dataset, sequences):\n",
        "  word_index, _ = tokenizing_dataset(original_dataset)\n",
        "  reversed_index = dict(map(reversed, word_index.items()))\n",
        "  for letter in range(len(sequences)):\n",
        "      reversed_index.get(letter) \n",
        "  reversed_sequence = [reversed_index.get(letter) for letter in sequences]\n",
        "  return (' '.join(reversed_sequence))\n",
        "    \n",
        "print(decoding_sequences(pdf_dataset_data, train_sequences[0]))\n",
        "print(train_data[:1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR_BRBTRRPRj"
      },
      "source": [
        "# One-hot split of the Labels\n",
        "\n",
        "pdf_dataset_labels_onehot = pd.get_dummies(pdf_dataset_labels)\n",
        "print('Shape of label tensor:', pdf_dataset_labels_onehot.shape)\n",
        "\n",
        "train_labels_onehot =  pdf_dataset_labels_onehot[1:training_mark]\n",
        "val_labels_onehot = pdf_dataset_labels_onehot[training_mark:]\n",
        "\n",
        "print(train_labels_onehot.head(5))\n",
        "print(val_labels_onehot.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aupoa_fnowZf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(pdf_dataset_data,pdf_dataset_labels_onehot, test_size = 0.10, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bip8i0ExoDC2"
      },
      "source": [
        "vocabulary_size = 16245\n",
        "embedding_dim = 128\n",
        "max_length = 31\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<00V>'\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             \n",
        "    tf.keras.layers.Embedding(vocabulary_size, embedding_dim, input_length = max_length),\n",
        "    #tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.SpatialDropout1D(0.5),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5, activation ='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjTyKxhioDC3"
      },
      "source": [
        "num_epochs = 20\n",
        "batch_size = 1000\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs = num_epochs, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUeqH3pjo5w"
      },
      "source": [
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXm7bhU0Kqyo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-mDU-mRNao"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDQgL14B8YLJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}